{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronales informadas por la física (PINNs)\n",
    "\n",
    "### Descripción desde Towards to data science\n",
    "\n",
    "En las últimas décadas, las redes neuronales artificiales se han utilizado para resolver problemas en diversos dominios aplicados, como la visión por computadora, el procesamiento del lenguaje natural y muchos más. Recientemente, ha surgido otra aplicación muy prometedora en la comunidad de aprendizaje automático científico (ML): la solución de ecuaciones diferenciales parciales (PDEs) utilizando redes neuronales artificiales, con un enfoque comúnmente conocido como redes neuronales informadas por la física (PINNs). Las PINNs se introdujeron originalmente en el trabajo seminal en [1] y hoy en día ya no están limitadas a un tema puramente de investigación, sino que están ganando tracción en la industria, lo suficiente como para entrar en el famoso ciclo de sobreexpectación de Gartner para tecnologías emergentes en 2021.\n",
    "\n",
    "Las PDEs juegan un papel crucial en muchos campos de la ingeniería y la ciencia fundamental, que van desde la dinámica de fluidos hasta la ingeniería acústica y estructural. Los métodos de modelado de elementos finitos (FEM) son los solucionadores estándar empleados de manera ubicua en la industria. A pesar de su popularidad, los métodos FEM presentan algunas limitaciones, como su costo computacional para problemas industriales grandes (principalmente debido al tamaño de la malla requerida) y problemas para aprovechar fuentes de datos externas, como datos de sensores, para guiar la solución de las PDEs.\n",
    "\n",
    "El enfoque de las PINNs discutido en esta publicación se considera una alternativa prometedora a los métodos FEM para cubrir algunas de estas limitaciones. Este enfoque es bastante diferente del ML supervisado estándar. De hecho, en lugar de depender únicamente de los datos, utiliza las propiedades físicas de la PDE en sí para guiar el proceso de entrenamiento. Los puntos de datos conocidos se pueden agregar fácilmente sobre la función de pérdida basada en la física para acelerar la velocidad de entrenamiento.\n",
    "\n",
    "ENLACE: [Towards Data Science](https://towardsdatascience.com/solving-differential-equations-with-neural-networks-afdcf7b8bcc4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción desde el abstract del paper inicial\n",
    "\n",
    "Introducimos las redes neuronales informadas por la física (PINNs) – redes neuronales que se entrenan para resolver tareas de aprendizaje supervisado respetando cualquier ley de la física descrita por ecuaciones diferenciales parciales no lineales generales. En este trabajo, presentamos nuestros desarrollos en el contexto de la resolución de dos clases principales de problemas: solución basada en datos y descubrimiento basado en datos de ecuaciones diferenciales parciales. Dependiendo de la naturaleza y disposición de los datos disponibles, ideamos dos tipos distintos de algoritmos, a saber, modelos de tiempo continuo y modelos de tiempo discreto. El primer tipo de modelos forma una nueva familia de aproximadores de funciones espacio-temporales eficientes en términos de datos, mientras que el segundo tipo permite el uso de esquemas de integración temporal implícitos de Runge-Kutta con precisión arbitraria y un número ilimitado de etapas. La efectividad del marco propuesto se demuestra a través de una colección de problemas clásicos en fluidos, mecánica cuántica, sistemas de reacción-difusión y la propagación de ondas no lineales en aguas someras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descripción con textos anteriores y Gemini\n",
    "\n",
    "## Introducción al proyecto: Redes neuronales informadas por la física (PINNs)\n",
    "\n",
    "En los últimos años, las redes neuronales artificiales (ANNs) se han convertido en una poderosa herramienta para resolver problemas complejos en diversos dominios científicos y de ingeniería. Una aplicación particularmente prometedora es el uso de ANNs para resolver ecuaciones diferenciales parciales (EDPs), un enfoque conocido como redes neuronales informadas por la física (PINNs). Introducidas en [1], las PINNs ofrecen una alternativa convincente a los métodos tradicionales como el modelado por elementos finitos (FEM) para resolver EDPs. Si bien el FEM se usa ampliamente en la industria, puede ser computacionalmente costoso para problemas grandes y tener dificultades para incorporar fuentes de datos externas como las mediciones de sensores. Las PINNs abordan estas limitaciones al combinar el aprendizaje basado en datos con las leyes físicas rectoras encapsuladas en las EDPs mismas. Esto permite que las PINNs no solo aprendan de los datos disponibles, sino que también aprovechen la física inherente para guiar el proceso de entrenamiento. Este enfoque único tiene un gran potencial para diversos campos, que incluyen la dinámica de fluidos, la acústica, la ingeniería estructural y más.\n",
    "\n",
    "* **Dinámica de fluidos:** Simulación de flujo de fluidos, como el flujo de aire alrededor de un ala de avión o el flujo de agua en una tubería.\n",
    "* **Acústica:** Modelado de propagación de ondas sonoras, como el diseño de salas de conciertos o la reducción de ruido en motores.\n",
    "* **Ingeniería estructural:** Análisis de tensiones y deformaciones en estructuras, como puentes, edificios y aviones.\n",
    "* **Mecánica cuántica:** Simulación de sistemas cuánticos, como el comportamiento de átomos y moléculas.\n",
    "* **Sistemas de reacción-difusión:** Modelado de reacciones químicas y difusión, como el crecimiento de cristales o la corrosión de metales.\n",
    "* **Propagación de ondas de aguas poco profundas no lineales:** Simulación de olas en el océano, como olas generadas por tormentas o tsunamis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se implementará una PINN para resolver una EDO, en particular la de crecimiento logístico. El objetivo de esta implementación es realizar un primer acercamiento al problema y al método. El lineamiento de lo siguiente viene desde la siguiente [publicación](https://towardsdatascience.com/solving-differential-equations-with-neural-networks-afdcf7b8bcc4) , los comentarios y variaciones son elaboración propia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mantener las cosas simples, en esta publicación nos enfocamos en la ecuación diferencial logística, una famosa ecuación diferencial ordinaria de primer orden utilizada para modelar el crecimiento de la población:\n",
    "$$\n",
    "\\frac{df(T)}{dt} = Rf(t)(1-f(t))\n",
    "$$\n",
    "\n",
    "Aquí, la función \\( f(t) \\) representa la tasa de crecimiento de la población a lo largo del tiempo \\( t \\) y el parámetro \\( R \\) proporciona la tasa máxima de crecimiento de la población y afecta fuertemente la forma de la solución. Para especificar completamente la solución de esta ecuación, es necesario imponer una condición de frontera, por ejemplo, en \\( t = 0 \\) tal como:\n",
    "\n",
    "$$\n",
    "f(0) = 0.5 \n",
    "$$\n",
    "\n",
    "Aunque la solución de esta ecuación puede derivarse fácilmente de manera analítica, representa un campo de prueba simple para ilustrar cómo funcionan las PINNs. Todas las técnicas explicadas a continuación son aplicables de inmediato a ecuaciones diferenciales ordinarias y parciales más complejas. Sin embargo, para escenarios más complejos se necesitarán trucos adicionales para lograr una buena convergencia.\n",
    "\n",
    "Las PINNs se basan en dos propiedades fundamentales de las redes neuronales (NN):\n",
    "\n",
    "1. Se ha demostrado formalmente [2] que las redes neuronales son aproximadores universales de funciones. Por lo tanto, una red neuronal, siempre que sea lo suficientemente profunda y expresiva, puede aproximar cualquier función y, por lo tanto, también la solución para la ecuación diferencial anterior.\n",
    "2. Es fácil y económico calcular las derivadas (de cualquier orden) de la salida de una red neuronal con respecto a cualquiera de sus entradas (y, por supuesto, a los parámetros del modelo durante la retropropagación) utilizando la diferenciación automática (AD). De hecho, la AD es lo que hizo que las redes neuronales fueran tan eficientes y exitosas en primer lugar.\n",
    "\n",
    "Estas son características interesantes, pero ¿cómo podemos hacer que la red neuronal realmente aprenda la solución? Y aquí viene la idea sorprendentemente simple pero extremadamente ingeniosa detrás de las PINNs [3, 4]: Podemos construir la función de pérdida de la red neuronal de manera que, cuando se minimice, la PDE se satisfaga automáticamente. En otras palabras, la contribución más importante de la pérdida se toma como el residuo de la ecuación diferencial de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\frac{df_{NN}(T)}{dt} - Rf_{NN}(t)(1-f_{NN}(t)) = 0\n",
    "$$\n",
    "\n",
    "donde $f_{NN}(t)$ es la salida de una red neuronal con una entrada y su derivada se calcula utilizando AD. Es inmediato ver que si la salida de la red neuronal respeta la ecuación anterior, uno está realmente resolviendo la ecuación diferencial. Para calcular la contribución real de la pérdida proveniente del residuo de la DE, es necesario especificar un conjunto de puntos en el dominio de la ecuación (generalmente conocidos como puntos de colocación) y evaluar el error cuadrático medio (MSE) u otra función de pérdida como un promedio sobre todos los puntos de colocación elegidos:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} _{DE}= \\frac{1}{M} \\sum_{j=1}^M\\left( \\frac{df_{NN}(t)}{dt}\\Big|_{t_{j}} - Rf_{NN}(t_j)(1-f_{NN}(t_j)) \\right)^2\n",
    "$$\n",
    "\n",
    "Sin embargo, una pérdida basada solo en el residuo anterior no garantiza tener una solución única para la ecuación. Por lo tanto, incluyamos la condición de frontera añadiéndola al cálculo de la pérdida de la misma manera que antes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{BC} = (f_{NN}(t_0)-0.5)^2 \\hspace{5pt} \\text{con } t_0 = 0\n",
    "$$\n",
    "\n",
    "Por lo tanto, la forma final de la función de pérdida será:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\mathcal{L}_{DE} + \\mathcal{L}_{BC}\n",
    "$$\n",
    "\n",
    "El marco de trabajo de PINNs es muy flexible y, utilizando las ideas presentadas anteriormente, uno puede agregar más condiciones de frontera, incluir condiciones más complejas como restricciones en las derivadas de $f(x)$, o tratar problemas dependientes del tiempo y multidimensionales utilizando una red neuronal con múltiples entradas.\n",
    "\n",
    "Veamos ahora cómo construir dicha función de pérdida con una red neuronal simple construida con PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torchopt\n",
    "from collections import OrderedDict\n",
    "from torch.func import functional_call, grad, vmap\n",
    "import scipy.io\n",
    "import warnings\n",
    "import plotly.graph_objects as go\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de PINNs para una EDO con Pytorch\n",
    "\n",
    "class LinearNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_inputs: int = 1,\n",
    "        num_layers: int = 1,\n",
    "        num_neurons: int = 5,\n",
    "        act: nn.Module = nn.Tanh(),\n",
    "    ) -> None:\n",
    "        \"\"\"Arquitectura básica de red neuronal con capas lineales\n",
    "        \n",
    "        Args:\n",
    "            num_inputs (int, opcional): la dimensionalidad del tensor de entrada\n",
    "            num_layers (int, opcional): el número de capas ocultas\n",
    "            num_neurons (int, opcional): el número de neuronas para cada capa oculta\n",
    "            act (nn.Module, opcional): la función de activación no lineal a utilizar para conectar\n",
    "                las capas lineales\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # capa de entrada\n",
    "        layers.append(nn.Linear(self.num_inputs, num_neurons))\n",
    "\n",
    "        # capas ocultas con capa lineal y activación\n",
    "        for _ in range(num_layers):\n",
    "            layers.extend([nn.Linear(num_neurons, num_neurons), act])\n",
    "\n",
    "        # capa de salida\n",
    "        layers.append(nn.Linear(num_neurons, 1))\n",
    "\n",
    "        # construir la red\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x.reshape(-1, 1)).squeeze()\n",
    "    \n",
    "def make_forward_fn(\n",
    "    model: nn.Module,\n",
    "    derivative_order: int = 1,\n",
    "    ) -> list[Callable]:\n",
    "        \"\"\"Crear una función de pase hacia adelante y funciones de gradiente para un modelo dado\n",
    "\n",
    "        Esta función crea un conjunto de llamadas funcionales del modelo de entrada\n",
    "    \n",
    "        Devuelve una lista de versiones v-mapeadas composables del pase hacia adelante\n",
    "        y de derivadas de orden superior con respecto a las entradas según lo especificado\n",
    "        por el argumento de entrada `derivative_order`\n",
    "    \n",
    "        Args:\n",
    "            model (nn.Module): el modelo para crear las llamadas funcionales. Puede ser cualquier subclase de\n",
    "                nn.Module\n",
    "            derivative_order (int, opcional): Hasta qué orden se devuelven las funciones para calcular la\n",
    "                derivada del modelo con respecto a las entradas\n",
    "    \n",
    "        Returns:\n",
    "            list[Callable]: Una lista de funciones donde cada elemento corresponde a\n",
    "                una versión v-mapeada del pase hacia adelante del modelo y sus derivadas. El\n",
    "                0-ésimo elemento es siempre el pase hacia adelante y, dependiendo del valor del\n",
    "                argumento `derivative_order`, los siguientes elementos corresponden a\n",
    "                la función de derivada de i-ésimo orden con respecto a las entradas del modelo. El\n",
    "                vmap asegura soporte eficiente para entradas en lotes\n",
    "        \"\"\"\n",
    "        def f(x: torch.Tensor, params: dict[str, torch.nn.Parameter] | tuple[torch.nn.Parameter, ...]) -> torch.Tensor:\n",
    "            if isinstance(params, tuple):\n",
    "                params_dict = tuple_to_dict_parameters(model, params)\n",
    "            else:\n",
    "                params_dict = params\n",
    "            return functional_call(model, params_dict, (x, ))\n",
    "\n",
    "        fns = []\n",
    "        fns.append(f)\n",
    "\n",
    "        dfunc = f\n",
    "        for _ in range(derivative_order):\n",
    "            dfunc = grad(dfunc)\n",
    "            dfunc_vmap = vmap(dfunc, in_dims=(0, None))\n",
    "            fns.append(dfunc_vmap)\n",
    "        return fns\n",
    "\n",
    "def tuple_to_dict_parameters(\n",
    "        model: nn.Module, params: tuple[torch.nn.Parameter, ...]\n",
    ") -> OrderedDict[str, torch.nn.Parameter]:\n",
    "    \"\"\"Convertir un conjunto de parámetros almacenados como una tupla en una forma de diccionario\n",
    "\n",
    "    Esta conversión es necesaria para poder llamar a la API `functional_call` que requiere\n",
    "    parámetros en forma de diccionario a partir de los resultados de un paso de optimización funcional que \n",
    "    devuelve los parámetros como una tupla\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): el modelo para crear las llamadas funcionales. Puede ser cualquier subclase de\n",
    "            nn.Module\n",
    "        params (tuple[Parameter, ...]): los parámetros del modelo almacenados como una tupla\n",
    "    \n",
    "    Returns:\n",
    "        Una instancia de OrderedDict con los parámetros almacenados como un diccionario ordenado\n",
    "    \"\"\"\n",
    "    keys = list(dict(model.named_parameters()).keys())\n",
    "    values = list(params)\n",
    "    return OrderedDict(({k:v for k,v in zip(keys, values)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las PINNs son un área de investigación muy activa y se han ideado arquitecturas de redes neuronales mucho más complejas y a menudo adaptadas a problemas específicos. La discusión sobre estas arquitecturas está fuera de esta parte introductoria.\n",
    "\n",
    "Construir la función de pérdida:\n",
    "\n",
    "Ahora que hemos definido nuestro aproximador universal de funciones, vamos a construir la función de pérdida. Como se discutió, esta se compone del término residual de la ecuación diferencial, que actúa como una regularización informada por la física, y el término de la condición de frontera, que guía a la red para converger a la solución deseada entre las infinitas posibles.\n",
    "\n",
    "Primero, es necesario elegir un conjunto de puntos de colocación. Dado que estamos resolviendo un problema muy simple, podemos elegir una cuadrícula uniformemente espaciada en el dominio del tiempo: `t = torch.linspace(0, 1, steps=30, requires_grad=True)` o podemos muestrear aleatoriamente nuevos puntos de colocación del dominio del tiempo en cada iteración del optimizador. Para problemas más complejos, la elección de puntos de colocación es extremadamente importante y puede afectar fuertemente los resultados.\n",
    "\n",
    "Para calcular la salida del modelo y su derivada estamos utilizando la API funcional de PyTorch, que hace que los modelos sean totalmente funcionales al desacoplar los parámetros del propio modelo.\n",
    "\n",
    "El enfoque funcional en PyTorch es muy conveniente cuando se trata de derivadas (de orden superior) de la salida de la red neuronal con respecto a sus entradas, como es a menudo el caso para las PINNs. En el código a continuación, usamos la API `torch.func` introducida en PyTorch 2.0 para construir un pase hacia adelante funcional y cálculos de gradiente de orden superior con soporte para lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import functional_call, grad, vmap\n",
    "\n",
    "model = LinearNN()\n",
    "\n",
    "# note que `functional_call` soporta entradas en lotes por defecto\n",
    "# por lo tanto, no es necesario llamar a vmap en él, como es el caso\n",
    "# para las llamadas a las derivadas\n",
    "def f(x: torch.Tensor, params: dict[str, torch.nn.Parameter]) -> torch.Tensor:\n",
    "    return functional_call(model, params_dict, (x, ))\n",
    "\n",
    "# función para calcular gradientes de orden superior con respecto\n",
    "# a la entrada simplemente componiendo llamadas `grad` y usando nuevamente `vmap` para\n",
    "# un lote eficiente de la entrada\n",
    "dfdx = vmap(grad(f), in_dims=(0, None))\n",
    "d2fdx2 = vmap(grad(grad(f)), in_dims=(0, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que las llamadas a `grad` se pueden componer sin restricciones, permitiendo así calcular derivadas de cualquier orden con respecto a las entradas. Usando las funciones definidas anteriormente, la pérdida MSE se calcula fácilmente como una suma de la contribución de la ecuación diferencial en cada punto de colocación y la contribución de la condición de frontera. Dado que el forward pass y las derivadas son de naturaleza funcional, la función de pérdida también debe tomar los parámetros del modelo como argumento de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 1.0  # tasa de crecimiento máximo de la población que parametriza la ecuación\n",
    "X_BOUNDARY = 0.0  # coordenada de la condición de frontera\n",
    "F_BOUNDARY = 0.5  # valor de la condición de frontera\n",
    "\n",
    "def make_loss_fn(f: Callable, dfdx: Callable) -> Callable:\n",
    "    \"\"\"Crear una función de evaluación de pérdida\n",
    "\n",
    "    La pérdida se calcula como la suma de la pérdida MSE interior (el residuo de la ecuación diferencial)\n",
    "    y el MSE de la pérdida en la frontera\n",
    "\n",
    "    Args:\n",
    "        f (Callable): El pase hacia adelante funcional del modelo utilizado como aproximador universal de funciones. Esta\n",
    "            es una función con la firma (x, params) donde `x` son los datos de entrada y `params` los parámetros del modelo.\n",
    "        dfdx (Callable): El cálculo funcional del gradiente del aproximador universal de funciones. Esta\n",
    "            es una función con la firma (x, params) donde `x` son los datos de entrada y `params` los parámetros del modelo.\n",
    "\n",
    "    Returns:\n",
    "        Callable: La función de pérdida con la firma (params, x) donde `x` son los datos de entrada y `params` los parámetros\n",
    "            del modelo. Nota que una simple llamada a `dloss = functorch.grad(loss_fn)` daría el gradiente\n",
    "            de la pérdida con respecto a los parámetros del modelo necesarios para los optimizadores.\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_fn(params: torch.Tensor, x: torch.Tensor):\n",
    "        f_value = f(x, params)\n",
    "        interior = dfdx(x, params) - R * f_value * (1 - f_value)\n",
    "        x0 = X_BOUNDARY\n",
    "        f0 = F_BOUNDARY\n",
    "        x_boundary = torch.tensor([x0])\n",
    "        f_boundary = torch.tensor([f0])\n",
    "        boundary = f(x_boundary, params) - f_boundary\n",
    "\n",
    "        loss = nn.MSELoss()\n",
    "        loss_value = loss(interior, torch.zeros_like(interior)) + loss(\n",
    "            boundary, torch.zeros_like(boundary)\n",
    "        )\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijamos la arquitectura del MLP\n",
    "num_hidden = 5\n",
    "dim_hidden = 5\n",
    "batch_size = 30\n",
    "num_iter = 150\n",
    "learning_rate = 0.1\n",
    "domain = (-5.0, 5.0)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configuración\n",
    "model = LinearNN(num_layers=num_hidden, num_neurons=dim_hidden, num_inputs=1)\n",
    "funcs = make_forward_fn(model, derivative_order=1)\n",
    "\n",
    "f = funcs[0]\n",
    "dfdx = funcs[1]\n",
    "loss_fn = make_loss_fn(f, dfdx)\n",
    "\n",
    "# adam como optimizador\n",
    "optimizer = torchopt.FuncOptimizer(torchopt.adam(lr=learning_rate))\n",
    "\n",
    "# Parámetros iniciales inicializados aleatoriamente\n",
    "params = tuple(model.parameters())\n",
    "\n",
    "# Entrenamos el modelo\n",
    "loss_evolution = []\n",
    "for i in range(num_iter):\n",
    "    # Muestreamos puntos en el dominio aleatoriamente para cada época\n",
    "    x = torch.FloatTensor(batch_size).uniform_(domain[0], domain[1])\n",
    "\n",
    "    # Calculamos la pérdida con los parámetros actuales\n",
    "    loss = loss_fn(params, x)\n",
    "\n",
    "    # Actualizamos los parámetros con el optimizador funcional\n",
    "    params = optimizer.step(loss, params)\n",
    "\n",
    "    print(f\"Iteración {i} con pérdida {float(loss)}\")\n",
    "    loss_evolution.append(float(loss))\n",
    "\n",
    "# Graficamos la solución en el dominio dado\n",
    "x_eval = torch.linspace(domain[0], domain[1], steps=100).reshape(-1, 1)\n",
    "f_eval = f(x_eval, params)\n",
    "analytical_sol_fn = lambda x: 1.0 / (1.0 + (1.0/F_BOUNDARY - 1.0) * np.exp(-R * x))\n",
    "x_eval_np = x_eval.detach().numpy()\n",
    "x_sample_np = torch.FloatTensor(batch_size).uniform_(domain[0], domain[1]).detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x_sample_np, analytical_sol_fn(x_sample_np), color=\"red\", label=\"Puntos de entrenamiento muestreados\")\n",
    "ax.plot(x_eval_np, f_eval.detach().numpy(), label=\"Solución final de PINN\")\n",
    "ax.plot(\n",
    "    x_eval_np,\n",
    "    analytical_sol_fn(x_eval_np),\n",
    "    label=f\"Solución analítica\",\n",
    "    color=\"green\",\n",
    "    alpha=0.75,\n",
    ")\n",
    "ax.set(title=\"Ecuación logística resuelta con NNs\", xlabel=\"t\", ylabel=\"f(t)\")\n",
    "ax.legend()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(loss_evolution)\n",
    "ax.set(title=\"Evolución de la pérdida\", xlabel=\"# épocas\", ylabel=\"Pérdida\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Eso es todo! La pérdida personalizada definida anteriormente asegura que después del procedimiento de entrenamiento, la red neuronal aproximará la solución a la ecuación diferencial elegida. Ahora, veámosla en acción.\n",
    "\n",
    "Resolviendo la ecuación diferencial con PINNs\n",
    "PyTorch no ofrece actualmente soporte nativo para optimizadores con la API funcional que usamos. Sin embargo, la increíble comunidad de PyTorch viene al rescate y se puede obtener una versión funcional de la mayoría de los optimizadores de PyTorch utilizando la biblioteca `torchopt`. La interfaz de esta biblioteca es muy intuitiva y resultará inmediatamente familiar para cualquier usuario de PyTorch. A continuación se muestra un bucle de entrenamiento básico para nuestro modelo funcional. Ten en cuenta que muestreamos aleatoriamente el dominio de la solución en cada iteración.\n",
    "\n",
    "Veamos algunos resultados. Usamos el optimizador Adam con una tasa de aprendizaje de 0.1 con 30 puntos de entrenamiento muestreados uniformemente del dominio en cada época de optimización. Dada la simplicidad de la ecuación diferencial elegida, 100 épocas son suficientes para reproducir casi perfectamente el resultado analítico con una tasa de crecimiento máximo fijada en $R = 1$:\n",
    "\n",
    "Aquí resolvimos un problema muy simple y unidimensional. Con ecuaciones más complejas, la convergencia no se logra tan fácilmente. Particularmente para problemas dependientes del tiempo, se han ideado muchos trucos útiles en los últimos años, como descomponer el dominio de la solución en diferentes partes resueltas utilizando diferentes redes neuronales, ponderación inteligente de diferentes contribuciones de pérdida para evitar converger a soluciones triviales y muchos más. Introduciré algunos de estos trucos en publicaciones futuras, así que mantente atento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se implementará una PINN para aproximar la ecuación de Burger. ###Comentar adecuadamente esta parte, en caso de ser útil\n",
    "\n",
    "Link del [repositorio](https://github.com/HridayM25/Physics-Informed-NN) con el código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lo siguiente se resolverá una EDP de la misma manera que se hizo antes, además de incluir en el código un parámetro que permita comparar para este caso el desempeño de un perceptrón multicapa normal con una PINN.\n",
    "\n",
    "En este caso tenemos que la ecuación de Burger viene dada por:\n",
    "\\begin{align*}\n",
    "u_t+uu_x -\\frac{0.01}{\\pi}u_{xx} &= 0 \\hspace{5pt} \\text{con } x\\in[-1,1], t\\in[0,1]\\\\\n",
    "u(0,x) &= -sin(\\pi x)\\\\\n",
    "u(t,-1) &= u(t,1) = 0\n",
    "\\end{align*}\n",
    "\n",
    "Ahora, sea $f := u_t+uu_t-\\frac{0.01}{\\pi}u_{xx}$, y con ello la función de error a minimizar es la siguiente:\n",
    "\\begin{align*}\n",
    "MSE &= MSE_u + MSE_f\\\\\n",
    "MSE_u &= \\frac{1}{N_u} \\sum_{i=1}^{N_u}|u(t_{iu},x_{iu})-u_i|^2\\\\\n",
    "MSE_f &= \\frac{1}{N_f} \\sum_{i=1}^{N_f}|f(t_{if},x_{if})|^2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "El MSE_u corresponde a los datos iniciales y de borde, mientras que MSE_f impone la estructura impuesta por la ecuación de Burgers en un conjunto finito de puntos de colocación. Los puntos t, x con un subíndice u denotan los datos de entrenamiento iniciales y de límites en u(t, x) y los puntos t,x con un subíndice f especifican los puntos de colocación para f(t, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comenzamos cargando la data \n",
    "import scipy.io\n",
    "\n",
    "#Cargamos la data\n",
    "data = scipy.io.loadmat('burgers_shock.mat')\n",
    "x = data['x'].flatten()[:, None]\n",
    "t = data['t'].flatten()[:, None]\n",
    "usol = np.real(data['usol']).T\n",
    "X, T = np.meshgrid(x, t)\n",
    "train = torch.concat([torch.Tensor(X.flatten()[:, None]), torch.Tensor(T.flatten()[:, None])], 1)\n",
    "X_min = train.min(0)\n",
    "X_max = train.max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Cargamos los datos desde un archivo .mat\n",
    "data = scipy.io.loadmat('burgers_shock.mat')\n",
    "x = data['x'].flatten()[:, None]  # Coordenadas espaciales\n",
    "t = data['t'].flatten()[:, None]  # Coordenadas temporales\n",
    "usol = np.real(data['usol']).T  # Solución de la ecuación de Burgers, transpuesta para ajuste de dimensiones\n",
    "\n",
    "# Creamos las mallas para la visualización\n",
    "X, T = np.meshgrid(x, t)\n",
    "\n",
    "# Convertimos los datos a un formato adecuado para el entrenamiento\n",
    "train = torch.cat([torch.Tensor(X.flatten()[:, None]), torch.Tensor(T.flatten()[:, None])], 1)\n",
    "X_min = train.min(0)[0]  # Valor mínimo de las características\n",
    "X_max = train.max(0)[0]  # Valor máximo de las características\n",
    "\n",
    "# Gráfico de superficie 3D de la solución usol\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "surf = ax.plot_surface(X, T, usol, cmap='viridis')\n",
    "ax.set_xlabel('X')  # Etiqueta del eje X\n",
    "ax.set_ylabel('T')  # Etiqueta del eje T\n",
    "ax.set_zlabel('usol')  # Etiqueta del eje Z\n",
    "ax.set_title('Gráfico de Superficie 3D de usol')\n",
    "#fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
    "\n",
    "# Mapa de calor de la solución usol\n",
    "ax2 = fig.add_subplot(122)\n",
    "c = ax2.pcolormesh(X, T, usol, shading='auto', cmap='viridis')\n",
    "ax2.set_xlabel('X')  # Etiqueta del eje X\n",
    "ax2.set_ylabel('T')  # Etiqueta del eje T\n",
    "ax2.set_title('Mapa de Calor de usol')\n",
    "fig.colorbar(c, ax=ax2)  # Barra de color para el mapa de calor\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arriba podemos redefinir estas variables por el nombre que tienen,\n",
    "# o cambiarlas en el resto del código, lo dejo así momentáneamente\n",
    "# ante posibles correcciones de código\n",
    "X_star = train  # Puntos de entrenamiento en el dominio\n",
    "u_star = usol  # Solución de la ecuación en los puntos de entrenamiento\n",
    "lb = X_min  # Límite inferior del dominio\n",
    "ub = X_max  # Límite superior del dominio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red neuronal para la solución u\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 16)  # Capa totalmente conectada con 2 entradas y 16 neuronas\n",
    "        self.fc2 = nn.Linear(16, 32)  # Capa totalmente conectada con 16 entradas y 32 neuronas\n",
    "        self.fc3 = nn.Linear(32, 1)  # Capa de salida con 32 entradas y 1 neurona\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))  # Activación ReLU después de la primera capa\n",
    "        x = nn.functional.relu(self.fc2(x))  # Activación ReLU después de la segunda capa\n",
    "        x = self.fc3(x)  # Salida de la red\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En esta implementación, se tiene que physics es un parámetro para agregar (o no)\n",
    "# los residuos a la función de pérdida\n",
    "class PINN():\n",
    "    def __init__(self, X, u, lb, ub, physics):\n",
    "        self.lb = torch.tensor(lb).float()\n",
    "        self.ub = torch.tensor(ub).float()\n",
    "        self.physics = physics\n",
    "        \n",
    "        self.x = torch.tensor(X[:, 0:1], requires_grad=True).float()  # Coordenadas espaciales con gradientes requeridos\n",
    "        self.t = torch.tensor(X[:, 1:2], requires_grad=True).float()  # Coordenadas temporales con gradientes requeridos\n",
    "        self.u = torch.tensor(u).float()  # Solución de la ecuación\n",
    "        \n",
    "        self.network = Network()  # Instancia de la red neuronal\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=0.001)  # Optimizador Adam\n",
    "        \n",
    "    def makeNetwork(self, x, t):\n",
    "        X = torch.cat([x, t], 1)  # Concatenamos las entradas espaciales y temporales\n",
    "        return self.network(X)\n",
    "    \n",
    "    def residual(self, x, t):\n",
    "        u = self.makeNetwork(x, t)\n",
    "        u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]  # Derivada de u respecto al tiempo\n",
    "        u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]  # Derivada de u respecto al espacio\n",
    "        u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]  # Segunda derivada de u respecto al espacio\n",
    "        \n",
    "        return u_t + u * u_x - (0.01 / np.pi) * u_xx  # Residuo de la ecuación diferencial\n",
    "    \n",
    "    def lossResidual(self):\n",
    "        u_pred = self.makeNetwork(self.x, self.t)\n",
    "        residual_pred = self.residual(self.x, self.t)\n",
    "        loss = torch.mean((self.u - u_pred)**2)  # Pérdida MSE entre la solución real y la predicha\n",
    "        if self.physics:  # Si physics == True agregamos el término de física a la función de pérdida\n",
    "            loss += torch.mean(residual_pred**2)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        lossTracker = []\n",
    "        self.network.train()\n",
    "        for idx in range(epochs):\n",
    "            u_pred = self.makeNetwork(self.x, self.t)\n",
    "            residual_pred = self.residual(self.x, self.t)\n",
    "            loss = torch.mean((self.u - u_pred)**2)\n",
    "            if self.physics:\n",
    "                loss += torch.mean(residual_pred**2)\n",
    "            lossTracker.append(loss.item())\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.step(self.lossResidual)\n",
    "        return lossTracker\n",
    "            \n",
    "    def predict(self): \n",
    "        self.network.eval()\n",
    "        u = self.makeNetwork(self.x, self.t)\n",
    "        res = self.residual(self.x, self.t)\n",
    "        return u.detach().numpy(), res.detach().numpy()  # Devolvemos las predicciones y los residuos como arrays de numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos aleatoriamente 2000 puntos de X_star sin reemplazo\n",
    "idx = np.random.choice(X_star.shape[0], 2000, replace=False)\n",
    "\n",
    "# Extraemos los puntos de entrenamiento correspondientes a los índices seleccionados\n",
    "X_u_train = X_star[idx, :]\n",
    "\n",
    "# Extraemos los valores de u correspondientes a los puntos de entrenamiento\n",
    "u_train = u_star.flatten()[:, None][idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos el modelo PINN con los puntos de entrenamiento y los límites del dominio\n",
    "# Aquí se define physics como True\n",
    "model = PINN(X_u_train, u_train, lb[0], ub[0], True)\n",
    "\n",
    "# Entrenamos el modelo PINN por 1000 épocas\n",
    "pinn = model.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos el modelo PINN con los puntos de entrenamiento y los límites del dominio\n",
    "# Aquí se define physics como False\n",
    "model = PINN(X_u_train, u_train, lb[0], ub[0], False)\n",
    "\n",
    "# Entrenamos el modelo PINN por 1000 épocas\n",
    "no_pinn = model.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(len(pinn)))\n",
    "\n",
    "# Crear una figura utilizando plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Añadir traza para la evolución de la pérdida del PINN\n",
    "fig.add_trace(go.Scatter(x=epochs, y=pinn, mode='lines', name='Physics Informed Neural Network'))\n",
    "\n",
    "# Actualizar el diseño del gráfico\n",
    "fig.update_layout(\n",
    "    title='Pérdida vs. Épocas PINN',\n",
    "    xaxis=dict(title='Épocas'),\n",
    "    yaxis=dict(title='Pérdida'),\n",
    "    legend=dict(x=0.7, y=1.0),\n",
    "    margin=dict(l=20, r=20, t=40, b=20),\n",
    "    hovermode='x unified' \n",
    ")\n",
    "\n",
    "# Mostrar el gráfico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(len(no_pinn)))\n",
    "\n",
    "# Crear una figura utilizando plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Añadir traza para la evolución de la pérdida del MLP sin información física\n",
    "fig.add_trace(go.Scatter(x=epochs, y=no_pinn, mode='lines', name='Not Physics Informed Neural Network'))\n",
    "\n",
    "# Actualizar el diseño del gráfico\n",
    "fig.update_layout(\n",
    "    title='Pérdida vs. Épocas MLP',\n",
    "    xaxis=dict(title='Épocas'),\n",
    "    yaxis=dict(title='Pérdida'),\n",
    "    legend=dict(x=0.7, y=1.0),\n",
    "    margin=dict(l=20, r=20, t=40, b=20),\n",
    "    hovermode='x unified' \n",
    ")\n",
    "\n",
    "# Mostrar el gráfico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que el error es consistentemente mayor en el primer gráfico, que corresponde a la función de de pérdida en la PINN. Con ello, tenemos entonces que en este caso para la red neuronal le es más fácil aproximar la solución sin imponer los residuos en la función de Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso EDP que no converge la PINN al aumentar el n\n",
    "\n",
    "Código desde Tarea Nico y chatgpt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Dimensión de input\n",
    "dim_input = 2\n",
    "\n",
    "# Dimensión de output\n",
    "dim_output = 1\n",
    "\n",
    "# Iteraciones del optimizador\n",
    "iterations = 3000\n",
    "\n",
    "# Número de onda\n",
    "n = 1\n",
    "k0 = 2 * np.pi * n\n",
    "\n",
    "# Geometría del dominio\n",
    "geom = [0, 1, 0, 1]\n",
    "\n",
    "# Condición de frontera\n",
    "def boundary(x):\n",
    "    return (x[:, 0] == 0) | (x[:, 0] == 1) | (x[:, 1] == 0) | (x[:, 1] == 1)\n",
    "\n",
    "# Dado que fijaremos la condición de borde, no se la daremos a la red directamente\n",
    "bc = []\n",
    "\n",
    "# Red neuronal totalmente conectada (FNN)\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, layers, activation):\n",
    "        super(FNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.layers.append(nn.Linear(layers[i], layers[i+1]))\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = self.activation(self.layers[i](x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "# Transformación para imponer la condición de borde\n",
    "def transform(x, y):\n",
    "    res = x[:, 0:1] * (1 - x[:, 0:1]) * x[:, 1:2] * (1 - x[:, 1:2])\n",
    "    return res * y\n",
    "\n",
    "# Ecuación en derivadas parciales (PDE)\n",
    "def pde(x, y, net):\n",
    "    y = net(x)\n",
    "    dy_x = torch.autograd.grad(y, x, grad_outputs=torch.ones_like(y), create_graph=True)[0]\n",
    "    dy_xx = torch.autograd.grad(dy_x[:, 0], x, grad_outputs=torch.ones_like(dy_x[:, 0]), create_graph=True)[0][:, 0]\n",
    "    dy_yy = torch.autograd.grad(dy_x[:, 1], x, grad_outputs=torch.ones_like(dy_x[:, 1]), create_graph=True)[0][:, 1]\n",
    "    f = k0 ** 2 * torch.sin(k0 * x[:, 0:1]) * torch.sin(k0 * x[:, 1:2])\n",
    "    return -dy_xx - dy_yy - k0 ** 2 * y - f\n",
    "\n",
    "# Generación de datos de entrenamiento\n",
    "num_domain = 500\n",
    "x_train = np.random.rand(num_domain, dim_input)\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Parámetros de la red\n",
    "wide, n_layers, activation = 100, 3, torch.sin\n",
    "layers = [dim_input] + [wide] * n_layers + [dim_output]\n",
    "\n",
    "net = FNN(layers, activation)\n",
    "\n",
    "# Optimización\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = transform(x_train, net(x_train))\n",
    "    loss = pde(x_train, y_pred, net).pow(2).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_history.append(loss.item())\n",
    "    if (i + 1) % 500 == 0:\n",
    "        print(f\"Iteration {i + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(iterations), loss_history)\n",
    "plt.xlabel(\"Iteraciones de entrenamiento\")\n",
    "plt.ylabel(\"Función de pérdida\")\n",
    "plt.title(f\"Evolución de la función de pérdida, n={n}\")\n",
    "plt.show()\n",
    "\n",
    "# Visualización de la solución\n",
    "def u_exact(x, y):\n",
    "    return np.sin(k0 * x) * np.sin(k0 * y)\n",
    "\n",
    "Nx, Ny = 100, 100\n",
    "x, y = np.linspace(0, 1, Nx), np.linspace(0, 1, Ny)\n",
    "\n",
    "arrays = [np.vstack((x, np.ones(Nx) * y[i])) for i in range(Ny)]\n",
    "X_ = np.hstack(arrays)\n",
    "X_ = torch.tensor(X_.T, dtype=torch.float32)\n",
    "\n",
    "u = transform(X_, net(X_)).detach().numpy()\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "U_exact = u_exact(X, Y)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "c1 = ax[0].contourf(X, Y, u.reshape((Nx, Ny)), levels=20, cmap='jet')\n",
    "ax[0].set_title(f\"Solución PINNs, n={n}\")\n",
    "\n",
    "divider = make_axes_locatable(ax[0])\n",
    "cax = divider.append_axes('right', size='3%', pad=0.02)\n",
    "fig.colorbar(c1, cax=cax, orientation='vertical')\n",
    "\n",
    "c2 = ax[1].contourf(X, Y, U_exact, levels=20, cmap='jet')\n",
    "ax[1].set_title(f\"Solución analítica, n={n}\")\n",
    "\n",
    "divider = make_axes_locatable(ax[1])\n",
    "cax = divider.append_axes('right', size='3%', pad=0.02)\n",
    "fig.colorbar(c2, cax=cax, orientation='vertical')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Dimensión de input\n",
    "dim_input = 2\n",
    "\n",
    "# Dimensión de output\n",
    "dim_output = 1\n",
    "\n",
    "# Iteraciones del optimizador\n",
    "iterations = 3000\n",
    "\n",
    "# Geometría del dominio\n",
    "geom = [0, 1, 0, 1]\n",
    "\n",
    "# Condición de frontera\n",
    "def boundary(x):\n",
    "    return (x[:, 0] == 0) | (x[:, 0] == 1) | (x[:, 1] == 0) | (x[:, 1] == 1)\n",
    "\n",
    "# Red neuronal totalmente conectada (FNN)\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, layers, activation):\n",
    "        super(FNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.layers.append(nn.Linear(layers[i], layers[i+1]))\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = self.activation(self.layers[i](x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "# Transformación para imponer la condición de borde\n",
    "def transform(x, y):\n",
    "    res = x[:, 0:1] * (1 - x[:, 0:1]) * x[:, 1:2] * (1 - x[:, 1:2])\n",
    "    return res * y\n",
    "\n",
    "# Ecuación en derivadas parciales (PDE)\n",
    "def pde(x, y, net, k0):\n",
    "    y = net(x)\n",
    "    dy_x = torch.autograd.grad(y, x, grad_outputs=torch.ones_like(y), create_graph=True)[0]\n",
    "    dy_xx = torch.autograd.grad(dy_x[:, 0], x, grad_outputs=torch.ones_like(dy_x[:, 0]), create_graph=True)[0][:, 0]\n",
    "    dy_yy = torch.autograd.grad(dy_x[:, 1], x, grad_outputs=torch.ones_like(dy_x[:, 1]), create_graph=True)[0][:, 1]\n",
    "    f = k0 ** 2 * torch.sin(k0 * x[:, 0:1]) * torch.sin(k0 * x[:, 1:2])\n",
    "    return -dy_xx - dy_yy - k0 ** 2 * y - f\n",
    "\n",
    "# Generación de datos de entrenamiento\n",
    "num_domain = 500\n",
    "x_train = np.random.rand(num_domain, dim_input)\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Parámetros de la red\n",
    "wide, n_layers, activation = 100, 3, torch.sin\n",
    "layers = [dim_input] + [wide] * n_layers + [dim_output]\n",
    "\n",
    "# Valores de n a considerar\n",
    "n_values = [1, 3, 5]\n",
    "\n",
    "for n in n_values:\n",
    "    k0 = 2 * np.pi * n\n",
    "    net = FNN(layers, activation)\n",
    "    \n",
    "    # Optimización\n",
    "    optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = transform(x_train, net(x_train))\n",
    "        loss = pde(x_train, y_pred, net, k0).pow(2).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_history.append(loss.item())\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"n={n}, Iteration {i + 1}, Loss: {loss.item()}\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(iterations), loss_history)\n",
    "    plt.xlabel(\"Iteraciones de entrenamiento\")\n",
    "    plt.ylabel(\"Función de pérdida\")\n",
    "    plt.title(f\"Evolución de la función de pérdida, n={n}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualización de la solución\n",
    "    def u_exact(x, y, k0):\n",
    "        return np.sin(k0 * x) * np.sin(k0 * y)\n",
    "    \n",
    "    Nx, Ny = 100, 100\n",
    "    x, y = np.linspace(0, 1, Nx), np.linspace(0, 1, Ny)\n",
    "    \n",
    "    arrays = [np.vstack((x, np.ones(Nx) * y[i])) for i in range(Ny)]\n",
    "    X_ = np.hstack(arrays)\n",
    "    X_ = torch.tensor(X_.T, dtype=torch.float32)\n",
    "    \n",
    "    u = transform(X_, net(X_)).detach().numpy()\n",
    "    \n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    U_exact = u_exact(X, Y, k0)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    c1 = ax[0].contourf(X, Y, u.reshape((Nx, Ny)), levels=20, cmap='jet')\n",
    "    ax[0].set_title(f\"Solución PINNs, n={n}\")\n",
    "    \n",
    "    divider = make_axes_locatable(ax[0])\n",
    "    cax = divider.append_axes('right', size='3%', pad=0.02)\n",
    "    fig.colorbar(c1, cax=cax, orientation='vertical')\n",
    "    \n",
    "    c2 = ax[1].contourf(X, Y, U_exact, levels=20, cmap='jet')\n",
    "    ax[1].set_title(f\"Solución analítica, n={n}\")\n",
    "    \n",
    "    divider = make_axes_locatable(ax[1])\n",
    "    cax = divider.append_axes('right', size='3%', pad=0.02)\n",
    "    fig.colorbar(c2, cax=cax, orientation='vertical')\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se aplicará Fourier features sobre esta RNN.\n",
    "Descripción desde Perplexity AI:\n",
    "Las Fourier Features son una técnica utilizada en el contexto de las Physics-Informed Neural Networks (PINNs) para mejorar su capacidad de aprender funciones de alta frecuencia en dominios de baja dimensión[1][3].\n",
    "\n",
    "La idea principal es aplicar una transformación de características de Fourier a los puntos de entrada antes de pasarlos a la red neuronal. Esta transformación mapea los puntos de entrada a un espacio de características de mayor dimensión utilizando una matriz gaussiana aleatoria[1].\n",
    "\n",
    "Esto tiene varios beneficios:\n",
    "\n",
    "1. Permite a la red neuronal aprender funciones de alta frecuencia que de otra manera serían difíciles de capturar[1][3]. \n",
    "\n",
    "2. Transforma el kernel tangente neural (NTK) de la red en un kernel estacionario con un ancho de banda ajustable[1]. Esto da un mejor control sobre el espectro de la función recuperada.\n",
    "\n",
    "3. Ayuda a las PINNs a converger más rápidamente y a evitar quedar atrapadas en mínimos locales o puntos de silla[3][5].\n",
    "\n",
    "En resumen, las Fourier Features son una forma efectiva de incorporar información de frecuencia a las entradas de las PINNs, lo que mejora significativamente su capacidad de aprender funciones multi-escala y su rendimiento general en la resolución de ecuaciones diferenciales parciales[1][3][5].\n",
    "\n",
    "Citations:\n",
    "[1] https://bmild.github.io/fourfeat/\n",
    "[2] https://www.mdpi.com/2311-5521/8/12/323\n",
    "[3] https://www.sciencedirect.com/science/article/abs/pii/S0045782521002759\n",
    "[4] https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4586276\n",
    "[5] https://www.researchgate.net/publication/370009039_On_the_use_of_Fourier_Features-Physics_Informed_Neural_Networks_FF-PINN_for_forward_and_inverse_fluid_mechanics_problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Dimensión de input\n",
    "dim_input = 2\n",
    "\n",
    "# Dimensión de output\n",
    "dim_output = 1\n",
    "\n",
    "# Iteraciones del optimizador\n",
    "iterations = 3000\n",
    "\n",
    "# Geometría del dominio\n",
    "geom = [0, 1, 0, 1]\n",
    "\n",
    "# Condición de frontera\n",
    "def boundary(x):\n",
    "    return (x[:, 0] == 0) | (x[:, 0] == 1) | (x[:, 1] == 0) | (x[:, 1] == 1)\n",
    "\n",
    "# Red neuronal totalmente conectada (FNN)\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, layers, activation):\n",
    "        super(FNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.layers.append(nn.Linear(layers[i], layers[i+1]))\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = self.activation(self.layers[i](x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "# Transformación para imponer la condición de borde\n",
    "def transform(x, y):\n",
    "    res = x[:, 0:1] * (1 - x[:, 0:1]) * x[:, 1:2] * (1 - x[:, 1:2])\n",
    "    return res * y\n",
    "\n",
    "# Ecuación en derivadas parciales (PDE)\n",
    "def pde(x, y, net, k0):\n",
    "    y = net(x)\n",
    "    dy_x = torch.autograd.grad(y, x, grad_outputs=torch.ones_like(y), create_graph=True)[0]\n",
    "    dy_xx = torch.autograd.grad(dy_x[:, 0], x, grad_outputs=torch.ones_like(dy_x[:, 0]), create_graph=True)[0][:, 0]\n",
    "    dy_yy = torch.autograd.grad(dy_x[:, 1], x, grad_outputs=torch.ones_like(dy_x[:, 1]), create_graph=True)[0][:, 1]\n",
    "    f = k0 ** 2 * torch.sin(k0 * x[:, 0:1]) * torch.sin(k0 * x[:, 1:2])\n",
    "    return -dy_xx - dy_yy - k0 ** 2 * y - f\n",
    "\n",
    "# Generación de datos de entrenamiento\n",
    "num_domain = 1000\n",
    "x_train = np.random.rand(num_domain, dim_input)\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Parámetros de la red\n",
    "wide, n_layers, activation = 100, 3, torch.tanh\n",
    "layers = [dim_input] + [wide] * n_layers + [dim_output]\n",
    "\n",
    "# Valores de n a considerar\n",
    "n_values = [1, 3, 5]\n",
    "\n",
    "for n in n_values:\n",
    "    k0 = 2 * np.pi * n\n",
    "    net = FNN(layers, activation)\n",
    "    \n",
    "    # Optimización\n",
    "    optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = transform(x_train, net(x_train))\n",
    "        loss = pde(x_train, y_pred, net, k0).pow(2).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_history.append(loss.item())\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"n={n}, Iteration {i + 1}, Loss: {loss.item()}\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(iterations), loss_history)\n",
    "    plt.xlabel(\"Iteraciones de entrenamiento\")\n",
    "    plt.ylabel(\"Función de pérdida\")\n",
    "    plt.title(f\"Evolución de la función de pérdida, n={n}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualización de la solución\n",
    "    def u_exact(x, y, k0):\n",
    "        return np.sin(k0 * x) * np.sin(k0 * y)\n",
    "    \n",
    "    Nx, Ny = 100, 100\n",
    "    x, y = np.linspace(0, 1, Nx), np.linspace(0, 1, Ny)\n",
    "    \n",
    "    arrays = [np.vstack((x, np.ones(Nx) * y[i])) for i in range(Ny)]\n",
    "    X_ = np.hstack(arrays)\n",
    "    X_ = torch.tensor(X_.T, dtype=torch.float32)\n",
    "    \n",
    "    u = transform(X_, net(X_)).detach().numpy()\n",
    "    \n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    U_exact = u_exact(X, Y, k0)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    c1 = ax[0].contourf(X, Y, u.reshape((Nx, Ny)), levels=20, cmap='jet')\n",
    "    ax[0].set_title(f\"Solución PINNs, n={n}\")\n",
    "    \n",
    "    divider = make_axes_locatable(ax[0])\n",
    "    cax = divider.append_axes('right', size='3%', pad=0.02)\n",
    "    fig.colorbar(c1, cax=cax, orientation='vertical')\n",
    "    \n",
    "    c2 = ax[1].contourf(X, Y, U_exact, levels=20, cmap='jet')\n",
    "    ax[1].set_title(f\"Solución analítica, n={n}\")\n",
    "    \n",
    "    divider = make_axes_locatable(ax[1])\n",
    "    cax = divider.append_axes('right', size='3%', pad=0.02)\n",
    "    fig.colorbar(c2, cax=cax, orientation='vertical')\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generación de datos de entrenamiento\n",
    "def generate_training_data(num_domain):\n",
    "    x_train = np.random.rand(num_domain, dim_input)\n",
    "    x_train = torch.tensor(x_train, dtype=torch.float32, requires_grad=True)\n",
    "    return x_train\n",
    "\n",
    "# Parámetros de la red\n",
    "wide, n_layers = 100, 3\n",
    "layers = [dim_input] + [wide] * n_layers + [dim_output]\n",
    "\n",
    "# Combinaciones de puntos de colocación y funciones de activación\n",
    "point_combinations = [500, 1000, 1500]\n",
    "activation_functions = [torch.tanh, torch.relu] #es posible agregar torch.sigmoid\n",
    "\n",
    "for num_points in point_combinations:\n",
    "    for activation in activation_functions:\n",
    "        x_train = generate_training_data(num_points)\n",
    "        \n",
    "        for n in n_values:\n",
    "            k0 = 2 * np.pi * n\n",
    "            net = FNN(layers, activation)\n",
    "            \n",
    "            # Optimización\n",
    "            optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "            \n",
    "            loss_history = []\n",
    "            \n",
    "            for i in range(iterations):\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = transform(x_train, net(x_train))\n",
    "                loss = pde(x_train, y_pred, net, k0).pow(2).mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_history.append(loss.item())\n",
    "                if (i + 1) % 500 == 0:\n",
    "                    print(f\"Points={num_points}, Activation={activation.__name__}, n={n}, Iteration {i + 1}, Loss: {loss.item()}\")\n",
    "            \n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(range(iterations), loss_history)\n",
    "            plt.xlabel(\"Iteraciones de entrenamiento\")\n",
    "            plt.ylabel(\"Función de pérdida\")\n",
    "            plt.title(f\"Puntos={num_points}, Activación={activation.__name__}, n={n}\")\n",
    "            plt.show()\n",
    "            \n",
    "            # Visualización de la solución\n",
    "            def u_exact(x, y, k0):\n",
    "                return np.sin(k0 * x) * np.sin(k0 * y)\n",
    "            \n",
    "            Nx, Ny = 100, 100\n",
    "            x, y = np.linspace(0, 1, Nx), np.linspace(0, 1, Ny)\n",
    "            \n",
    "            arrays = [np.vstack((x, np.ones(Nx) * y[i])) for i in range(Ny)]\n",
    "            X_ = np.hstack(arrays)\n",
    "            X_ = torch.tensor(X_.T, dtype=torch.float32)\n",
    "            \n",
    "            u = transform(X_, net(X_)).detach().numpy()\n",
    "            \n",
    "            X, Y = np.meshgrid(x, y)\n",
    "            \n",
    "            U_exact = u_exact(X, Y, k0)\n",
    "            \n",
    "            fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "            \n",
    "            c1 = ax[0].contourf(X, Y, u.reshape((Nx, Ny)), levels=20, cmap='jet')\n",
    "            ax[0].set_title(f\"Solución PINNs, n={n}\")\n",
    "            \n",
    "            divider = make_axes_locatable(ax[0])\n",
    "            cax = divider.append_axes('right', size='3%', pad=0.02)\n",
    "            fig.colorbar(c1, cax=cax, orientation='vertical')\n",
    "            \n",
    "            c2 = ax[1].contourf(X, Y, U_exact, levels=20, cmap='jet')\n",
    "            ax[1].set_title(f\"Solución analítica, n={n}\")\n",
    "            \n",
    "            divider = make_axes_locatable(ax[1])\n",
    "            cax = divider.append_axes('right', size='3%', pad=0.02)\n",
    "            fig.colorbar(c2, cax=cax, orientation='vertical')\n",
    "            \n",
    "            plt.subplots_adjust(wspace=0.3)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá hay que correr el código y elegir la mejor combinación, luego a esa combinación de puntos de colocación y función de activación se le aplicará Fourier features, en el siguiente código: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Este código al parecer no está funcionando bien, quizás hay que corregir la implementación del problema (dice chatgpt)\n",
    "## Está indicado donde aplicar los cambios a los puntos de colocación y la función de activación\n",
    "\n",
    "\n",
    "# Dimensión de input\n",
    "dim_input = 2\n",
    "\n",
    "# Dimensión de output\n",
    "dim_output = 1\n",
    "\n",
    "# Iteraciones del optimizador\n",
    "iterations = 3000\n",
    "\n",
    "# Geometría del dominio\n",
    "geom = [0, 1, 0, 1]\n",
    "\n",
    "# Fourier Features\n",
    "class FourierFeatures(nn.Module):\n",
    "    def __init__(self, input_dim, mapping_size=256, scale=10.0):\n",
    "        super(FourierFeatures, self).__init__()\n",
    "        self.B = torch.randn((input_dim, mapping_size)) * scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_proj = 2.0 * np.pi * x @ self.B\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "\n",
    "# Condición de frontera\n",
    "def boundary(x):\n",
    "    return (x[:, 0] == 0) | (x[:, 0] == 1) | (x[:, 1] == 0) | (x[:, 1] == 1)\n",
    "\n",
    "# Red neuronal totalmente conectada (FNN)\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, layers, activation):\n",
    "        super(FNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.layers.append(nn.Linear(layers[i], layers[i+1]))\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = self.activation(self.layers[i](x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "# Transformación para imponer la condición de borde\n",
    "def transform(x, y):\n",
    "    res = x[:, 0:1] * (1 - x[:, 0:1]) * x[:, 1:2] * (1 - x[:, 1:2])\n",
    "    return res * y\n",
    "\n",
    "# Ecuación en derivadas parciales (PDE)\n",
    "def pde(x, y, net, k0):\n",
    "    x_ff = fourier_features(x)  # Transformar x usando FourierFeatures\n",
    "    y = net(x_ff)\n",
    "    dy_x = torch.autograd.grad(y, x, grad_outputs=torch.ones_like(y), create_graph=True)[0]\n",
    "    dy_xx = torch.autograd.grad(dy_x[:, 0], x, grad_outputs=torch.ones_like(dy_x[:, 0]), create_graph=True)[0][:, 0]\n",
    "    dy_yy = torch.autograd.grad(dy_x[:, 1], x, grad_outputs=torch.ones_like(dy_x[:, 1]), create_graph=True)[0][:, 1]\n",
    "    f = k0 ** 2 * torch.sin(k0 * x[:, 0:1]) * torch.sin(k0 * x[:, 1:2])\n",
    "    return -dy_xx - dy_yy - k0 ** 2 * y - f\n",
    "\n",
    "# Generación de datos de entrenamiento\n",
    "num_domain = 1000  # Ajustar puntos de colocación al optimo encontrado antes\n",
    "x_train = np.random.rand(num_domain, dim_input)\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Parámetros de la red\n",
    "wide, n_layers, activation = 100, 4, torch.relu  # Cambiar la función de activación a la optima encontrada antes\n",
    "mapping_size = 256  # Definir el tamaño de mapeo de Fourier Features\n",
    "layers = [2 * mapping_size] + [wide] * n_layers + [dim_output]  # Ajustar capas según Fourier Features\n",
    "\n",
    "# Valores de n a considerar\n",
    "n_values = [1, 3, 5]\n",
    "\n",
    "for n in n_values:\n",
    "    k0 = 2 * np.pi * n\n",
    "    fourier_features = FourierFeatures(dim_input, mapping_size=mapping_size)\n",
    "    net = FNN(layers, activation)\n",
    "    \n",
    "    # Inicialización Xavier\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    \n",
    "    # Optimización\n",
    "    optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "        x_ff = fourier_features(x_train)\n",
    "        y_pred = transform(x_train, net(x_ff))\n",
    "        loss = pde(x_train, y_pred, net, k0).pow(2).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_history.append(loss.item())\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"n={n}, Iteration {i + 1}, Loss: {loss.item()}\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(iterations), loss_history)\n",
    "    plt.xlabel(\"Iteraciones de entrenamiento\")\n",
    "    plt.ylabel(\"Función de pérdida\")\n",
    "    plt.title(f\"Evolución de la función de pérdida, n={n}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualización de la solución\n",
    "    def u_exact(x, y, k0):\n",
    "        return np.sin(k0 * x) * np.sin(k0 * y)\n",
    "    \n",
    "    Nx, Ny = 100, 100\n",
    "    x, y = np.linspace(0, 1, Nx), np.linspace(0, 1, Ny)\n",
    "    \n",
    "    arrays = [np.vstack((x, np.ones(Nx) * y[i])) for i in range(Ny)]\n",
    "    X_ = np.hstack(arrays)\n",
    "    X_ = torch.tensor(X_.T, dtype=torch.float32)\n",
    "    \n",
    "    x_ff = fourier_features(X_)\n",
    "    u = transform(X_, net(x_ff)).detach().numpy()\n",
    "    \n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    U_exact = u_exact(X, Y, k0)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    c1 = ax[0].contourf(X, Y, u.reshape((Nx, Ny)), levels=20, cmap='jet')\n",
    "    ax[0].set_title(f\"Solución PINNs, n={n}\")\n",
    "    \n",
    "    divider = make_axes_locatable(ax[0])\n",
    "    cax = divider.append_axes('right', size='3%', pad=0.02)\n",
    "    fig.colorbar(c1, cax=cax, orientation='vertical')\n",
    "    \n",
    "    c2 = ax[1].contourf(X, Y, U_exact, levels=20, cmap='jet')\n",
    "    ax[1].set_title(f\"Solución analítica, n={n}\")\n",
    "    \n",
    "    divider = make_axes_locatable(ax[1])\n",
    "    cax = divider.append_axes('right', size='3%', pad=0.02)\n",
    "    fig.colorbar(c2, cax=cax, orientation='vertical')\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
