{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Eikonal del tutorial de PINNs\n",
    "\n",
    "Supongamos que estamos analizando una región 2D recubierta por células del corazón. Algunas de ellas son marcapasos cardíacos, originando pulsos de propagación eléctrica hacia el resto del tejido. Usted dispone de múltiples electrodos en el dominio de observación, que le permiten determinar el tiempo en que una onda llega a cada uno de ellos.\n",
    "\n",
    "Los tiempos de llegada normalizados pueden ser modelados mediante la ecuación Eikonal:\n",
    "\n",
    "$$\\sqrt{\\nabla T \\cdot \\nabla T} = 1$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "from jax import random, grad, jit, vmap\n",
    "import jax\n",
    "from jax.scipy.optimize import minimize\n",
    "import numpy as onp\n",
    "from jax.example_libraries import optimizers\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt\n",
    "from pyDOE2 import lhs\n",
    "key = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(layers, key):\n",
    "\n",
    "  Ws = [] # matriz de pesos\n",
    "  bs = [] # vector de sesgos\n",
    "\n",
    "  # inicialización de Glorot\n",
    "  for i in range(len(layers) - 1):\n",
    "\n",
    "    std_glorot = np.sqrt(2/(layers[i] + layers[i + 1]))\n",
    "    key, subkey = random.split(key)\n",
    "    Ws.append(random.normal(subkey, (layers[i], layers[i + 1]))*std_glorot)\n",
    "    bs.append(np.zeros(layers[i + 1]))\n",
    "\n",
    "  return [Ws, bs]\n",
    "\n",
    "@jit\n",
    "def forward_pass(H, params):\n",
    "\n",
    "  Ws = params[0] # obtener pesos de la red\n",
    "  bs = params[1] # obtener sesgos\n",
    "\n",
    "  N_layers = len(Ws) # número total de capas (entrada + ocultas + salida)\n",
    "\n",
    "  for i in range(N_layers - 1):\n",
    "\n",
    "    H = np.matmul(H, Ws[i]) + bs[i] # pasar por aplicación lineal\n",
    "    H = np.tanh(H) # activación tangente hiperbólica\n",
    "\n",
    "  Y = np.matmul(H, Ws[-1]) + bs[-1] # pasar por capa de salida (sin función de activación)\n",
    "\n",
    "  return Y\n",
    "\n",
    "def create_grads():\n",
    "\n",
    "  fp_wrapper = lambda x, params: forward_pass(x, params)[0] # output del paso forward\n",
    "  du_dx = grad(fp_wrapper) # gradiente de jax\n",
    "\n",
    "  du_dx_wrapper = lambda x, params: du_dx(x, params)[0] # output del gradiente\n",
    "  du_dxx = grad(du_dx_wrapper)\n",
    "\n",
    "  dU_dx = vmap(du_dx, in_axes = (0, None), out_axes=0) # vectorización de du_dx en eje 0\n",
    "  dU_dxx = vmap(du_dxx, in_axes = (0, None), out_axes=0) # vectorización de du_dxx en eje 0\n",
    "\n",
    "  return dU_dx, dU_dxx\n",
    "\n",
    "grad_X, grad_XX = create_grads() # crear gradientes\n",
    "\n",
    "@partial(jit, static_argnums=(0,)) # definir argumentos que serán tratados como estáticos\n",
    "\n",
    "def step(loss, i, opt_state, X_batch, Y_batch, X_c, X_bd, X_bn):\n",
    "\n",
    "    params = get_params(opt_state) # obtener parámetros\n",
    "    g = grad(loss)(params, X_batch, Y_batch, X_c, X_bd, X_bn) # calcular gradiente de pérdida para el batch\n",
    "\n",
    "    return opt_update(i, g, opt_state)\n",
    "\n",
    "def train(loss, X, Y, X_c, opt_state, X_bd = None, X_bn = None, nIter = 10000):\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    for it in range(nIter): # iterador de épocas\n",
    "\n",
    "        opt_state = step(loss, it, opt_state, X, Y, X_c, X_bd, X_bn)\n",
    "\n",
    "        if it % 100 == 0: # imprimir estados\n",
    "\n",
    "            params = get_params(opt_state) # obtener parámetros\n",
    "            train_loss_value = loss(params, X, Y, X_c, X_bd, X_bn) # ver pérdida en dicha época\n",
    "            train_loss.append(train_loss_value) # agregar a lista\n",
    "\n",
    "            to_print = \"it %i, train loss = %e\" % (it, train_loss_value)\n",
    "            print(to_print)\n",
    "\n",
    "    return get_params(opt_state), train_loss, val_loss\n",
    "\n",
    "def train_bfgs(loss, params, x_train, u_train, x_c, x_bd = None, x_bn = None): # método bfgs\n",
    "\n",
    "    def concat_params(params):\n",
    "\n",
    "        flat_params, params_tree = jax.tree_util.tree_flatten(params)\n",
    "        params_shape = [x.shape for x in flat_params]\n",
    "\n",
    "        return np.concatenate([x.reshape(-1) for x in flat_params]), (params_tree, params_shape)\n",
    "\n",
    "    def reconstruct_params(param_vector, params_shape):\n",
    "\n",
    "        split_params = onp.split(param_vector, onp.cumsum([onp.prod(s) for s in params_shape[:-1]]))\n",
    "        flat_params = [x.reshape(s) for x, s in zip(split_params, params_shape)]\n",
    "        params = jax.tree_util.tree_unflatten(params_tree, flat_params)\n",
    "\n",
    "        return params\n",
    "\n",
    "    param_vector, (params_tree, params_shape) = concat_params(params)\n",
    "\n",
    "    @jit\n",
    "    def func(param_vector):\n",
    "      params = reconstruct_params(param_vector, params_shape)\n",
    "\n",
    "      return loss(params, x_train, u_train, x_c, x_bd, x_bn)\n",
    "\n",
    "    results = minimize(func, param_vector, method = 'bfgs')\n",
    "\n",
    "    print(results.fun)\n",
    "\n",
    "    return reconstruct_params(results.x, params_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(x, c = np.array([0.55,0.45])):\n",
    "  return np.linalg.norm(x - c)\n",
    "\n",
    "xs = ys = np.linspace(0,1,50)\n",
    "Xm, Ym = np.meshgrid(xs, ys)\n",
    "x_star = np.c_[Xm.ravel(), Ym.ravel()] # concatenar arreglos\n",
    "\n",
    "origin = np.array([0.55,0.45])\n",
    "N_train = 20\n",
    "\n",
    "x_train = lhs(2, N_train, random_state = 1234) # muestras aleatorias en el dominio\n",
    "u_train = vmap(F, in_axes = (0, None))(x_train,origin)[:,None]\n",
    "\n",
    "u_star = vmap(F, in_axes = (0, None))(x_star,origin)[:,None]\n",
    "\n",
    "plt.figure(dpi = 150)\n",
    "plt.contourf(Xm, Ym, u_star.reshape(Xm.shape))\n",
    "plt.colorbar()\n",
    "\n",
    "plt.plot(x_train[:,0],x_train[:,1], \"k.\", label = \"posición de electrodos\")\n",
    "plt.plot(origin[0], origin[1], \"r.\", label = \"marcapasos\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"tiempos de llegada\")\n",
    "\n",
    "plt.axis(\"equal\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefinir función de entrenamiento para generar puntos de colocación aleatorios en cada iteración\n",
    "\n",
    "def train(loss, X, Y, X_c_shape, opt_state, X_bd = None, X_bn = None, nIter = 10000):\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    for it in range(nIter):\n",
    "\n",
    "        X_c = lhs(X_c_shape[1], X_c_shape[0])\n",
    "        opt_state = step(loss, it, opt_state, X, Y, X_c, X_bd, X_bn)\n",
    "\n",
    "        if it % 100 == 0:\n",
    "\n",
    "            params = get_params(opt_state)\n",
    "            train_loss_value = loss(params, X, Y, X_c, X_bd, X_bn)\n",
    "            train_loss.append(train_loss_value)\n",
    "\n",
    "            to_print = \"it %i, train loss = %e\" % (it, train_loss_value)\n",
    "\n",
    "            print(to_print)\n",
    "\n",
    "    return get_params(opt_state), train_loss, val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [2, 100, 50, 20, 1]\n",
    "\n",
    "params = init_params(layers, key)\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.adam(1e-3)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "X_c_shape = (200,2) # number of collocations points\n",
    "\n",
    "@jit\n",
    "def loss(params, X, U, Xc, X_bd, X_bn):\n",
    "  MSE_data = np.average((forward_pass(X, params) - U)**2)\n",
    "  MSE_PDE = np.average((np.linalg.norm(grad_X(Xc,params),axis = 1)-1)**2)\n",
    "  return  MSE_data + MSE_PDE\n",
    "params, train_loss, val_loss = train(loss,x_train, u_train, X_c_shape, opt_state, nIter = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = forward_pass(x_star, params)\n",
    "pred_origin = x_star[np.argmin(u_pred)] #la predicción será dd llegue en tpo 0\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.contourf(Xm, Ym, u_pred.reshape(Xm.shape))\n",
    "\n",
    "plt.plot(x_train[:,0], x_train[:,1], \"k.\")\n",
    "plt.plot(origin[0], origin[1], \"r.\", label = \"origen\")\n",
    "plt.plot(pred_origin[0], pred_origin[1], \"b.\", label = \"origen pred.\")\n",
    "\n",
    "plt.title('tiempos de llegada predecidos')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "plt.colorbar()\n",
    "plt.axis('equal')\n",
    "plt.legend()\n",
    "\n",
    "print(f\"error: {onp.linalg.norm(pred_origin - origin)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación del código a PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad, Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyDOE2 import lhs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pde_loss(model, x_c):\n",
    "    x_c.requires_grad_(True)\n",
    "    u_c = model(x_c)\n",
    "    gradients = torch.autograd.grad(outputs=u_c, inputs=x_c, grad_outputs=torch.ones_like(u_c), create_graph=True)[0]\n",
    "    mse_pde = torch.mean((torch.linalg.norm(gradients, dim=1) - 1)**2)  # Suponiendo la EDP: |grad(u)| = 1\n",
    "    return mse_pde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, x_train, y_train, x_c, iterations=10000):\n",
    "    model.train()\n",
    "    for it in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "        u_pred = model(x_train)\n",
    "        mse_data = criterion(u_pred, y_train)\n",
    "        mse_pde = compute_pde_loss(model, x_c)\n",
    "\n",
    "        loss = mse_data + mse_pde\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if it % 100 == 0:\n",
    "            print(f\"Iteration {it}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de datos\n",
    "Datos aleatorios de la EDP para generar puntos de entrenamiento para la PINNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    # Definir dominio\n",
    "    xs = ys = np.linspace(0, 1, 50)\n",
    "    Xm, Ym = np.meshgrid(xs, ys)\n",
    "    x_star = np.hstack([Xm.ravel()[:, None], Ym.ravel()[:, None]])  # Dominio completo para predicción\n",
    "\n",
    "    origin = np.array([0.55, 0.45])  # Origen de la propagación\n",
    "    N_train = 20  # número de datos de entrenamiento\n",
    "\n",
    "    # Generar puntos de entrenamiento usando LHS (Latin Hypercube Sampling)\n",
    "    x_train = torch.tensor(lhs(2, N_train, criterion=\"center\"), dtype=torch.float32)\n",
    "    y_train = torch.sqrt((x_train[:, 0] - origin[0])**2 + (x_train[:, 1] - origin[1])**2).unsqueeze(1)\n",
    "\n",
    "    return x_train, y_train, x_star, Xm, Ym, origin\n",
    "\n",
    "x_train, y_train, x_star, Xm, Ym, origin = generate_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, x_train, y_train, x_c, iterations=10000):\n",
    "    model.train()\n",
    "    for it in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "        u_pred = model(x_train)\n",
    "        mse_data = criterion(u_pred, y_train)\n",
    "        mse_pde = compute_pde_loss(model, x_c)\n",
    "\n",
    "        loss = mse_data + mse_pde\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if it % 100 == 0:\n",
    "            print(f\"Iteration {it}, Loss: {loss.item()}\")\n",
    "\n",
    "# Datos de colocación para la EDP\n",
    "x_c = torch.rand(200, 2, dtype=torch.float32)  # Simulando puntos de colocación\n",
    "\n",
    "# Entrenar el modelo\n",
    "model = init_params([2, 100, 50, 20, 1])\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.MSELoss()\n",
    "train(model, optimizer, criterion, x_train, y_train, x_c)\n",
    "\n",
    "u_pred = model(torch.tensor(x_star, dtype=torch.float32)).detach().numpy().reshape(Xm.shape)\n",
    "pred_origin = x_star[np.argmin(u_pred)]\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.contourf(Xm, Ym, u_pred)\n",
    "plt.plot(x_train[:, 0], x_train[:, 1], \"k.\", label=\"Posición de electrodos\")\n",
    "plt.plot(origin[0], origin[1], \"r.\", label=\"Marcapasos\")\n",
    "plt.plot(pred_origin[0], pred_origin[1], \"b.\", label=\"Origen predicho\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Tiempos de llegada predecidos\")\n",
    "plt.colorbar()\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "print(f\"Error: {np.linalg.norm(pred_origin - origin)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
